"""*******************************************************************************
* Licensed Materials - Property of IBM
* (c) Copyright IBM Corporation ${year}. All Rights Reserved.
*
* Note to U.S. Government Users Restricted Rights:
* Use, duplication or disclosure restricted by GSA ADP Schedule
* Contract with IBM Corp.
*******************************************************************************"""

"""
Author: Utkarsh Desai
Maintenance: Utkarsh Desai
"""

import sys
import json
import utils
from metrics import Metrics
from monolith import Monolith

# Keys/names for metrics
coverage_key                = "coverage"
conceptual_independence_key = "conceptual_independence"
modularity_key              = "modularity"
structural_cohesivity_key   = "structural_cohesivity"
structural_modularity_key   = "structural_modularity"
ned_key                     = "ned"
connection_maintained_key   = "single_connections_maintained"
inconsistency_prop_key      = "inconsistency_proportion"
avg_microservice_size_key   = "avg_microservice_size"
avg_interface_count_key     = "avg_interface_count"
chattiness_key              = "chattiness"
cluster_purity_key          = "cluster_purity_on_entrypoints"
entrypoint_purity_key       = "entrypoint_purity_on_clusters"

class PartitionEvaluator(object):
    """
    Main class to trigger evaluation of partitions generated by CMA
    Refer to the end of the file for example usage
    """

    def __init__(self, partition_file, icu_file=None, callgraph_file=None, entrypoint_file=None, output_file=None, detail=False):
        self.output_file = output_file
        self.detail = detail

        with open(partition_file) as json_file:
            self.data = json.load(json_file)

        self._validate_schema()
        print("Schema validated.")

        self.app = None
        self.source_sink = None
        if icu_file is not None and callgraph_file is not None and entrypoint_file is not None:
            #print("Parsing ICU and Call graph .. ")
            self.app = Monolith(icu_file, callgraph_file, entrypoint_file)
            self.source_sink = utils.extract_unique_usage_relations(self.app.icu)

    def evaluate_clusters(self):
        """
        Evaluate if the generated partitions can be explained via some static analysis feature
        """
        if self.app is None:
            print("Unable to evaluate. Please provide ICU and Call-graph information.")
            return

        clusters, edges = utils.get_clusters_and_edges(self.data)
        num_clusters = len(clusters)

        # Store evaluation results for each cluster here:
        evaluation_results = []

        total_classes = 0.0
        total_inconsistencies = 0.0

        # Get list of clusters
        for clusterid in clusters.keys():
            cm = clusters[clusterid]
            members = cm['classes']
            all_members = list(members)

            total_classes += len(all_members)

            # Result and inconsistencies for this cluster
            result = {'clusterid':clusterid, 'members':all_members}
            inconsistencies = []

            ### Check for ICU link
            missing_icu_link_ids = []
            # for mem in all_members:
            #     print("\t", mem)

            all_idxs = [self.app.icuclass2idx[m] for m in all_members]  # all classes in this cluster
            found_idxs = [] # a list of classes we found have some relation with some other class in cluster
            for i in all_idxs:  # this loop denotes the class under consideration
                for j in all_idxs:  # this loop denotes all classes
                    if i == j:
                        continue
                    if self.app.icu[i,j] > 0 or self.app.icu[j,i] > 0 or self.app.implements[i,j]>0 or self.app.implements[j,i]>0:
                        found_idxs.append(i)
                        break
            # From all classes, if we have not found some class in link with another class, we capture it
            missing_icu_link_ids = [i for i in all_idxs if i not in found_idxs]
            missing_icu_link_classes = [self.app.idx2icuclass[idx] for idx in missing_icu_link_ids]
            if self.detail:
                print("*** Classes in cluster", clusterid, "with no ICU link - Total:", len(missing_icu_link_ids), missing_icu_link_classes)
            inconsistencies.append({'type':'ICU', 'classes':missing_icu_link_classes})

            ### Check for entrypoint link
            reverse_ep = utils.reverse_entrypoint(self.app.entrypoints)

            missing_ep_link_classes = []
            for i, ci in enumerate(all_members): # this loop denotes the class under consideration
                common_ep = 0
                for j,cj in enumerate(all_members):  # this loop denotes all classes
                    if ci == cj:
                        continue
                    ep_i = reverse_ep.get(ci)
                    ep_j = reverse_ep.get(cj)
                    if ep_i is None or ep_j is None:
                        continue
                    for ei in ep_i:
                        if ei in ep_j:
                            common_ep += 1
                            break
                if common_ep == 0:
                    missing_ep_link_classes.append(ci)
            if self.detail:
                print("+++ Classes in this cluster with no entrypoints common with other classes:", len(missing_ep_link_classes), missing_ep_link_classes)
            inconsistencies.append({'type':'EntryPoint', 'classes':missing_ep_link_classes})

            icu_and_ep_inconsitencies = [x for x in missing_icu_link_classes if x in missing_ep_link_classes]
            if self.detail:
                print("Both missing:", icu_and_ep_inconsitencies)

            num_inconsistencies = len(icu_and_ep_inconsitencies)
            total_inconsistencies += num_inconsistencies

            # Add to this cluster's result
            result['inconsistencies'] = inconsistencies
            # Add this clnuster's result to the glocal result for all clusters
            evaluation_results.append(result)

        evaluation_result = {'evaluation_results':evaluation_results}

        single_connections_maintained, total_single_conn = utils.get_maintained_connections_proportion(clusters, self.source_sink, self.app.icuclass2idx)

        evaluation_result[connection_maintained_key] = single_connections_maintained

        inconsistency_prop = total_inconsistencies/total_classes
        evaluation_result[inconsistency_prop_key] = inconsistency_prop

        avg_size = total_classes/num_clusters

        evaluation_result[avg_microservice_size_key] = avg_size

        avg_interface_count = utils.get_interface_count(self.app, clusters)
        evaluation_result[avg_interface_count_key] = avg_interface_count

        chattiness = utils.get_chattiness(self.app, clusters)
        evaluation_result[chattiness_key] = chattiness

        cluster_purity, entrypoint_purity = utils.compute_purity_metrics(self.app.entrypoints, reverse_ep, clusters)

        evaluation_result[cluster_purity_key] = cluster_purity
        evaluation_result[entrypoint_purity_key] = entrypoint_purity_key

        #if self.detail:
        print("Single connection maintained:", single_connections_maintained, "of total:", total_single_conn)
        print("Total inconsistencies detected:", inconsistency_prop, "Total classes:", total_classes, "Inconsistent:", total_inconsistencies)
        print("Num clusters:", num_clusters)
        print("Average size:", avg_size)
        print("Average Interface Count:", avg_interface_count)
        print("Chattiness", chattiness)
        print("Avg entrypoints per cluster:", cluster_purity)
        print("Avg clusters per entrypoint:", entrypoint_purity)

        print(str(num_clusters)+"\t"+str(self.result[conceptual_independence_key])+"\t"+str(self.result[modularity_key])+"\t"+str(self.result[structural_cohesivity_key])+"\t"
                    +str(self.result[structural_modularity_key])+"\t"+str(self.result[ned_key])+"\t"+str(single_connections_maintained)+" / "+str(total_single_conn) + "\t"
                    +str(inconsistency_prop)+" of "+str(total_classes)+"\t"+str(avg_size)+"\t"+str(avg_interface_count) + "\t"
                    +str(chattiness))

        #print("Num clusters:", len(clusters))
        return evaluation_result

    def compute_metrics(self):
        """
        Compute all available metrics for this partitioning
        """
        coverage = Metrics.get_coverage(self.data)
        conceptual_independence = Metrics.get_conceptual_independence(self.data)
        modularity = Metrics.get_modularity(self.data)
        structural_cohesivity = Metrics.get_structural_cohesivity(self.data)
        structural_modularity = Metrics.get_structural_modularity(self.data)
        ned = Metrics.get_ned(self.data)

        self.result = { coverage_key : coverage,
                        conceptual_independence_key : conceptual_independence,
                        modularity_key : modularity,
                        structural_cohesivity_key : structural_cohesivity,
                        structural_modularity_key : structural_modularity,
                        ned_key : ned
                    }
        #TODO: avg microcluster size will come here, since it doesnt really depend on ICU etc

        if self.output_file is not None:
            with open(self.output_file, 'w') as f:
                json.dump(self.result, f)
            print("Output written to file:", self.output_file)

        return self.result

    def _validate_schema(self):
        pass

if __name__ == "__main__":

    if len(sys.argv) < 2:
        print("Arguments: <partition-json-file> <(optional)output-file> <icufile(optional)> <callgraphfile(optional)> <entrypointfile(optional)>")
        sys.exit(1)

    inputfile = sys.argv[1]
    outputfile = None
    icufile = None
    callgraphfile = None
    entrypointfile = None

    if len(sys.argv) > 2:
        outputfile = sys.argv[2]
    if len(sys.argv) == 6:
        icufile =sys.argv[3]
        callgraphfile = sys.argv[4]
        entrypointfile = sys.argv[5]

    DETAIL_MODE = False

    eval = PartitionEvaluator(partition_file=inputfile,
                                output_file=outputfile,
                                icu_file=icufile,
                                callgraph_file=callgraphfile,
                                entrypoint_file=entrypointfile,
                                detail=DETAIL_MODE)

    metrics = eval.compute_metrics()
    print("Result:", metrics)
    eval_result = eval.evaluate_clusters()
    #print("Evaluation:", eval_result)
